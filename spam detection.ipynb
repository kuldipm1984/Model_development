{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "910b9fe6-6400-4fd4-9032-e1bdaa883eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e43e9d8e-5ef8-4f5f-8fde-0fcde43aa01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "403b0d41-251b-4748-bc24-700c55e2d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)  \n",
    "    text = re.sub('\"', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z?.!,\\d]', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    return text\n",
    "df['Message'] = df['Message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dba989b-1478-4b06-833b-a1491d385dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for input text\n",
    "msg_tokenizer = Tokenizer()\n",
    "msg_tokenizer.fit_on_texts(df['Message'])\n",
    "X_train = msg_tokenizer.texts_to_sequences(df['Message'])\n",
    "max_msg_length = pd.Series(X_train).map(len).max()+1\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_msg_length, padding='post')\n",
    "vocab_size = len(msg_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0833febd-9960-4b7d-9030-7e91a9d1b53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 191)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1efa62f4-fdb4-4602-b4ce-4dde0b0526d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_msg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff5cef67-f8ec-4a3c-be4c-c55674a1de61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5567    1\n",
       "5568    0\n",
       "5569    0\n",
       "5570    0\n",
       "5571    0\n",
       "Name: Category, Length: 5572, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "le=LabelEncoder()\n",
    "df['Category']= le.fit_transform(df['Category'])\n",
    "y=df['Category']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caa36dcf-3aee-4612-b830-b84f1028b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(seq_len, d_model)\n",
    "\n",
    "    def positional_encoding(self, seq_len, d_model):\n",
    "        position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        sin_vals = tf.math.sin(position * div_term)\n",
    "        cos_vals = tf.math.cos(position * div_term)\n",
    "        pos_encoding = tf.concat([sin_vals, cos_vals], axis=-1)\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure that positional encoding matches input dimensions\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        sequence_length = tf.shape(inputs)[1]\n",
    "        d_model = tf.shape(inputs)[2]  # Ensure matching feature size\n",
    "\n",
    "        pos_encoding_resized = self.pos_encoding[:sequence_length, :d_model]  # Adjust to match feature dimensions\n",
    "        return inputs + tf.expand_dims(pos_encoding_resized, axis=0)  # Expand for batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "375f97b2-8f93-4e9f-bd1a-99c0058b8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 191)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 191, 25)              217775    ['input_13[0][0]']            \n",
      "                                                                                                  \n",
      " positional_encoding_9 (Pos  (None, 191, 25)              0         ['embedding[0][0]']           \n",
      " itionalEncoding)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 191, 25)              20625     ['positional_encoding_9[0][0]'\n",
      " ltiHeadAttention)                                                  , 'positional_encoding_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 191, 25)              0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 191, 25)              0         ['positional_encoding_9[0][0]'\n",
      " OpLambda)                                                          , 'dropout_8[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 191, 25)              50        ['tf.__operators__.add_8[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 191, 256)             6656      ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 191, 25)              6425      ['dense_20[0][0]']            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6  (None, 25)                   0         ['dense_21[0][0]']            \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 1)                    26        ['global_average_pooling1d_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 251557 (982.64 KB)\n",
      "Trainable params: 251557 (982.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "d_model = 25 # Embedding size\n",
    "num_heads = 8  # Number of attention heads\n",
    "dff = 256  # Feedforward network size\n",
    "from tensorflow.keras import layers\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_msg_length,))  # (Batch, Time Steps, Features)\n",
    "encoder_embedding = Embedding(vocab_size, d_model, mask_zero=True)(encoder_inputs)\n",
    "pos_encoding_enc = PositionalEncoding(max_msg_length,d_model)\n",
    "encoder_inputs_with_pos = pos_encoding_enc(encoder_embedding)\n",
    "attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(encoder_inputs_with_pos, encoder_inputs_with_pos)\n",
    "attention = layers.Dropout(0.2)(attention)\n",
    "attention = layers.LayerNormalization(epsilon=1e-6)(encoder_inputs_with_pos + attention)\n",
    "outputs = layers.Dense(256, activation='relu')(attention)\n",
    "outputs = layers.Dense(d_model)(outputs)\n",
    "pooled_output = layers.GlobalAveragePooling1D()(outputs) # summarize\n",
    "classification_output = layers.Dense(1, activation='sigmoid')(pooled_output)\n",
    "model = Model(inputs=encoder_inputs, outputs=classification_output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da2e9942-7e5a-4a1a-8dc9-af156f06e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 33s 411ms/step - loss: 0.4137 - accuracy: 0.8649 - val_loss: 0.3907 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x201dfd27950>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y, epochs=1, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d77e069d-c587-4980-9ae7-8f9301197091",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= 'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8aa075c-dff6-4d6a-988b-7eeb19e0fd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)  \n",
    "    text = re.sub('\"', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z?.!,\\d]', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    return text\n",
    "test_input=preprocess_text(test)\n",
    "msg_tokenizer.fit_on_texts(['test_input'])\n",
    "seq = msg_tokenizer.texts_to_sequences(['test_input'])\n",
    "test_padded = pad_sequences(seq, maxlen=max_msg_length, padding='post')\n",
    "predicted_labels = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbdd2076-7952-4680-a16e-1dea5b1b2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = np.argmax(predicted_labels, axis=1)  # Get class index\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07241a6-69ff-40fe-ba54-88453771d84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
