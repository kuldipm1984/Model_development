{"cells":[{"cell_type":"code","execution_count":null,"id":"e2bf83c0-c0ed-421d-9f34-cb70a4da1f60","metadata":{"id":"e2bf83c0-c0ed-421d-9f34-cb70a4da1f60"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import re\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization"]},{"cell_type":"code","execution_count":null,"id":"408a2c80-0abc-4fc0-982c-0f4a4bf9dfd6","metadata":{"id":"408a2c80-0abc-4fc0-982c-0f4a4bf9dfd6"},"outputs":[],"source":["df = pd.read_csv(\"article_highlights.csv\")\n","df = df[['article', 'highlights']].dropna()"]},{"cell_type":"code","execution_count":null,"id":"ad70339d-43e0-4722-894d-3fcd775a2b2f","metadata":{"id":"ad70339d-43e0-4722-894d-3fcd775a2b2f"},"outputs":[],"source":["def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'\\([^)]*\\)', '', text)\n","    text = re.sub('\"', '', text)\n","    text = re.sub(r'[^a-zA-Z?.!,\\d]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text\n","\n","df['article'] = df['article'].apply(preprocess_text)\n","df['highlights'] = df['highlights'].apply(preprocess_text)"]},{"cell_type":"code","execution_count":null,"id":"1fb5e60d-2ec0-4e2b-8d02-db184f92c01c","metadata":{"id":"1fb5e60d-2ec0-4e2b-8d02-db184f92c01c"},"outputs":[],"source":["# Tokenizer for input text\n","text_tokenizer = Tokenizer()\n","text_tokenizer.fit_on_texts(df['article'])\n","X_train = text_tokenizer.texts_to_sequences(df['article'])\n","max_article_length = pd.Series(X_train).map(len).max()+1\n","X_train_padded = pad_sequences(X_train, maxlen=max_article_length, padding='post')"]},{"cell_type":"code","execution_count":null,"id":"88db8aff-d144-4294-84d6-0770948d9a5a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88db8aff-d144-4294-84d6-0770948d9a5a","executionInfo":{"status":"ok","timestamp":1742134309181,"user_tz":240,"elapsed":313,"user":{"displayName":"Kuldip Mitra","userId":"09334071575411676230"}},"outputId":"afb3d413-f448-4c62-b693-0d6f0f3e7882"},"outputs":[{"output_type":"stream","name":"stdout","text":["(8165, 57)\n"]}],"source":["#df['highlights'] = df['highlights'].apply(lambda s: f\"startofseq {s} endofseq\")\n","df['highlights'] = \"startofseq \" + df['highlights'] + \" endofseq\"\n","\n","summary_tokenizer = Tokenizer()\n","summary_tokenizer.fit_on_texts(df['highlights'])\n","Y_train = summary_tokenizer.texts_to_sequences(df['highlights'])\n","max_highlights_length = pd.Series(Y_train).map(len).max()+1\n","Y_train_padded = pad_sequences(Y_train, maxlen=max_highlights_length, padding='post')\n","print(Y_train_padded.shape)"]},{"cell_type":"code","execution_count":null,"id":"a8426538-c703-402c-b5be-42a0e52fbb1f","metadata":{"id":"a8426538-c703-402c-b5be-42a0e52fbb1f"},"outputs":[],"source":["text_vocab_size = len(text_tokenizer.word_index) + 1\n","summary_vocab_size = len(summary_tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":null,"id":"1d61222a-a7de-410d-bc03-197f5b536ee3","metadata":{"id":"1d61222a-a7de-410d-bc03-197f5b536ee3"},"outputs":[],"source":["from tensorflow.keras import layers\n","class PositionalEncoding(layers.Layer):\n","    def __init__(self, seq_len, d_model):\n","        super(PositionalEncoding, self).__init__()\n","        self.pos_encoding = self.positional_encoding(seq_len, d_model)\n","\n","    def positional_encoding(self, seq_len, d_model):\n","        position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n","        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n","        sin_vals = tf.math.sin(position * div_term)\n","        cos_vals = tf.math.cos(position * div_term)\n","        pos_encoding = tf.concat([sin_vals, cos_vals], axis=-1)\n","        return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","    def call(self, inputs):\n","        # Ensure that positional encoding matches input dimensions\n","        batch_size = tf.shape(inputs)[0]\n","        sequence_length = tf.shape(inputs)[1]\n","        d_model = tf.shape(inputs)[2]  # Ensure matching feature size\n","\n","        pos_encoding_resized = self.pos_encoding[:sequence_length, :d_model]  # Adjust to match feature dimensions\n","        return inputs + tf.expand_dims(pos_encoding_resized, axis=0)  # Expand for batch dimension\n"]},{"cell_type":"code","execution_count":null,"id":"54986042-eb88-495a-ab44-b04246d66ec1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54986042-eb88-495a-ab44-b04246d66ec1","executionInfo":{"status":"ok","timestamp":1742134326740,"user_tz":240,"elapsed":129,"user":{"displayName":"Kuldip Mitra","userId":"09334071575411676230"}},"outputId":"3a942199-95b0-4e3c-b99e-6afdb36deda5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'positional_encoding' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n","  warnings.warn(\n"]}],"source":["# Model parameters\n","d_model = 25 # Embedding size\n","num_heads = 8  # Number of attention heads\n","dff = 256  # Feedforward network size\n","from tensorflow.keras import layers\n","# Encoder\n","encoder_inputs = Input(shape=(X_train_padded.shape[1],))  # (Batch, Time Steps, Features)\n","encoder_embedding = Embedding(text_vocab_size, d_model, mask_zero=True)(encoder_inputs)\n","pos_encoding_enc = PositionalEncoding(max_article_length,d_model)\n","encoder_inputs_with_pos = pos_encoding_enc(encoder_embedding)\n","attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(encoder_inputs_with_pos, encoder_inputs_with_pos)\n","attention = layers.Dropout(0.2)(attention)\n","attention = layers.LayerNormalization(epsilon=1e-6)(encoder_inputs_with_pos + attention)\n","encoder_outputs = layers.Dense(256, activation='relu')(attention)\n","encoder_outputs = layers.Dense(d_model)(encoder_outputs)\n","encoder_outputs = layers.LayerNormalization(epsilon=1e-6)(attention + encoder_outputs)"]},{"cell_type":"code","execution_count":null,"id":"3e48a2d5-37ec-414b-88c8-24fadf8d28b9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e48a2d5-37ec-414b-88c8-24fadf8d28b9","executionInfo":{"status":"ok","timestamp":1742134328963,"user_tz":240,"elapsed":138,"user":{"displayName":"Kuldip Mitra","userId":"09334071575411676230"}},"outputId":"b1445a60-20bd-4c9d-84b9-412c569d694e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'positional_encoding_1' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n","  warnings.warn(\n"]}],"source":["# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = Embedding(summary_vocab_size, d_model, mask_zero=True)(decoder_inputs)\n","pos_encoding_dec = PositionalEncoding(max_highlights_length, d_model)\n","decoder_inputs_with_pos = pos_encoding_dec(decoder_embedding)\n","attention1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(decoder_inputs_with_pos, decoder_inputs_with_pos)\n","attention1 = layers.Dropout(0.2)(attention1)\n","attention1 = layers.LayerNormalization(epsilon=1e-6)(decoder_inputs_with_pos + attention1)\n","attention2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(attention1, encoder_outputs)  # Attend to encoder output\n","attention2 = layers.Dropout(0.2)(attention2)\n","attention2 = layers.LayerNormalization(epsilon=1e-6)(attention1 + attention2)\n","decoder_outputs = layers.Dense(256, activation='relu')(attention2)\n","decoder_outputs = layers.Dense(d_model)(decoder_outputs)\n","decoder_outputs = layers.Dropout(0.2)(decoder_outputs)\n","decoder_outputs = layers.LayerNormalization(epsilon=1e-6)(attention2 + decoder_outputs)"]},{"cell_type":"code","execution_count":null,"id":"2ae82c9f-906f-49c1-a74c-feb2338c49af","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2ae82c9f-906f-49c1-a74c-feb2338c49af","executionInfo":{"status":"ok","timestamp":1742134330729,"user_tz":240,"elapsed":89,"user":{"displayName":"Kuldip Mitra","userId":"09334071575411676230"}},"outputId":"959c4126-bb90-48cd-e610-826fc91f9fce"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │         \u001b[38;5;34m26,500\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ positional_encoding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │         \u001b[38;5;34m20,625\u001b[0m │ positional_encoding[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ positional_encoding[\u001b[38;5;34m0\u001b[0m… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │         \u001b[38;5;34m16,925\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ positional_encoding[\u001b[38;5;34m0\u001b[0m… │\n","│                           │                        │                │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ positional_encoding_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │             \u001b[38;5;34m50\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │         \u001b[38;5;34m20,625\u001b[0m │ positional_encoding_1… │\n","│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ positional_encoding_1… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │          \u001b[38;5;34m6,656\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │          \u001b[38;5;34m6,425\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ positional_encoding_1… │\n","│                           │                        │                │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n","│                           │                        │                │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │             \u001b[38;5;34m50\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │             \u001b[38;5;34m50\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │         \u001b[38;5;34m20,625\u001b[0m │ layer_normalization_2… │\n","│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n","│                           │                        │                │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │             \u001b[38;5;34m50\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │          \u001b[38;5;34m6,656\u001b[0m │ layer_normalization_3… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │          \u001b[38;5;34m6,425\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_3… │\n","│                           │                        │                │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │             \u001b[38;5;34m50\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m677\u001b[0m)      │         \u001b[38;5;34m17,602\u001b[0m │ layer_normalization_4… │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">26,500</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ positional_encoding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,625</span> │ positional_encoding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ positional_encoding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,925</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_encoding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                           │                        │                │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ positional_encoding_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,625</span> │ positional_encoding_1… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ positional_encoding_1… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,425</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_encoding_1… │\n","│                           │                        │                │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                           │                        │                │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,625</span> │ layer_normalization_2… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n","│                           │                        │                │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │ layer_normalization_3… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,425</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_3… │\n","│                           │                        │                │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">677</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17,602</span> │ layer_normalization_4… │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m149,314\u001b[0m (583.26 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,314</span> (583.26 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m149,314\u001b[0m (583.26 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,314</span> (583.26 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["final_outputs = layers.Dense(summary_vocab_size, activation='softmax')(decoder_outputs)  # Output layer\n","model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=final_outputs)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"f85ad07d-9158-4fdc-9d58-3a44d8f3622e","metadata":{"id":"f85ad07d-9158-4fdc-9d58-3a44d8f3622e"},"outputs":[],"source":["# decoder input and output data\n","decoder_input_data = Y_train_padded[:, :-1]\n","decoder_output_data = Y_train_padded[:, 1:]"]},{"cell_type":"code","execution_count":null,"id":"0477d7ef-7f06-4bcf-8d88-59e6b05d9801","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0477d7ef-7f06-4bcf-8d88-59e6b05d9801","executionInfo":{"status":"ok","timestamp":1742141721279,"user_tz":240,"elapsed":5684375,"user":{"displayName":"Kuldip Mitra","userId":"09334071575411676230"}},"outputId":"d2926d8f-1a24-4b85-9e81-0190db606300"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 220ms/step - accuracy: 0.5374 - loss: 4.2454\n","Epoch 2/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 216ms/step - accuracy: 0.8638 - loss: 1.0028\n","Epoch 3/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 221ms/step - accuracy: 0.9652 - loss: 0.3078\n","Epoch 4/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 217ms/step - accuracy: 0.9873 - loss: 0.1250\n","Epoch 5/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 219ms/step - accuracy: 0.9936 - loss: 0.0668\n","Epoch 6/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 219ms/step - accuracy: 0.9961 - loss: 0.0432\n","Epoch 7/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 222ms/step - accuracy: 0.9982 - loss: 0.0252\n","Epoch 8/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 227ms/step - accuracy: 0.9990 - loss: 0.0163\n","Epoch 9/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 217ms/step - accuracy: 0.9995 - loss: 0.0110\n","Epoch 10/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 216ms/step - accuracy: 0.9997 - loss: 0.0082\n","Epoch 11/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 221ms/step - accuracy: 0.9998 - loss: 0.0062\n","Epoch 12/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 220ms/step - accuracy: 0.9999 - loss: 0.0045\n","Epoch 13/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 219ms/step - accuracy: 1.0000 - loss: 0.0037\n","Epoch 14/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0028\n","Epoch 15/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 0.0022\n","Epoch 16/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 218ms/step - accuracy: 0.9999 - loss: 0.0023\n","Epoch 17/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 0.0014\n","Epoch 18/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 0.0012\n","Epoch 19/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 0.0011\n","Epoch 20/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0012\n","Epoch 21/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 9.6522e-04\n","Epoch 22/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 7.5582e-04\n","Epoch 23/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 217ms/step - accuracy: 0.9998 - loss: 0.0017\n","Epoch 24/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 8.2161e-04\n","Epoch 25/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 6.0657e-04\n","Epoch 26/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 3.6097e-04\n","Epoch 27/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 3.0608e-04\n","Epoch 28/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 2.4781e-04\n","Epoch 29/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 217ms/step - accuracy: 0.9999 - loss: 0.0015\n","Epoch 30/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 2.6108e-04\n","Epoch 31/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 1.8420e-04\n","Epoch 32/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 1.6753e-04\n","Epoch 33/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 1.6720e-04\n","Epoch 34/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 1.3690e-04\n","Epoch 35/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 217ms/step - accuracy: 0.9998 - loss: 0.0011\n","Epoch 36/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 219ms/step - accuracy: 1.0000 - loss: 2.0843e-04\n","Epoch 37/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 1.0855e-04\n","Epoch 38/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 8.4220e-05\n","Epoch 39/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 7.9123e-05\n","Epoch 40/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 5.6459e-05\n","Epoch 41/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 6.0341e-05\n","Epoch 42/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 0.9998 - loss: 0.0011\n","Epoch 43/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 1.4986e-04\n","Epoch 44/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 6.2896e-05\n","Epoch 45/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 2.0750e-04\n","Epoch 46/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 3.3123e-05\n","Epoch 47/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 2.7681e-05\n","Epoch 48/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 2.8305e-05\n","Epoch 49/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 220ms/step - accuracy: 0.9999 - loss: 4.7796e-04\n","Epoch 50/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 1.1428e-04\n","Epoch 51/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 4.6323e-05\n","Epoch 52/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 3.0232e-05\n","Epoch 53/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 1.2152e-04\n","Epoch 54/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 8.6383e-05\n","Epoch 55/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 7.3391e-05\n","Epoch 56/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 2.6454e-05\n","Epoch 57/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 1.4739e-04\n","Epoch 58/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 2.0922e-04\n","Epoch 59/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 212ms/step - accuracy: 0.9999 - loss: 2.7879e-04\n","Epoch 60/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 4.6806e-05\n","Epoch 61/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 2.8556e-05\n","Epoch 62/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 3.5058e-05\n","Epoch 63/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 3.0105e-05\n","Epoch 64/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 1.0034e-05\n","Epoch 65/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 1.5658e-05\n","Epoch 66/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 1.2242e-05\n","Epoch 67/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 1.4091e-04\n","Epoch 68/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 0.9998 - loss: 6.7134e-04\n","Epoch 69/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 8.6736e-05\n","Epoch 70/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 7.8333e-05\n","Epoch 71/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 1.2858e-05\n","Epoch 72/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 2.0623e-05\n","Epoch 73/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 1.6487e-05\n","Epoch 74/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 1.9037e-05\n","Epoch 75/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 1.6945e-04\n","Epoch 76/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 0.9999 - loss: 2.7629e-04\n","Epoch 77/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 2.8989e-05\n","Epoch 78/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 1.4757e-05\n","Epoch 79/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 1.4589e-05\n","Epoch 80/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 9.1215e-06\n","Epoch 81/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 3.3718e-05\n","Epoch 82/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 2.2179e-05\n","Epoch 83/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 1.4860e-05\n","Epoch 84/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 215ms/step - accuracy: 0.9999 - loss: 4.7511e-04\n","Epoch 85/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 3.8172e-05\n","Epoch 86/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 6.0710e-05\n","Epoch 87/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 3.2588e-05\n","Epoch 88/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 1.5292e-05\n","Epoch 89/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 2.7377e-05\n","Epoch 90/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 1.0650e-05\n","Epoch 91/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 8.3390e-05\n","Epoch 92/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 214ms/step - accuracy: 0.9999 - loss: 3.6203e-04\n","Epoch 93/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 1.4853e-04\n","Epoch 94/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 1.8616e-05\n","Epoch 95/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 2.3832e-05\n","Epoch 96/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 219ms/step - accuracy: 1.0000 - loss: 1.2872e-05\n","Epoch 97/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 1.1685e-05\n","Epoch 98/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 4.0430e-06\n","Epoch 99/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 2.1192e-05\n","Epoch 100/100\n","\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 214ms/step - accuracy: 0.9999 - loss: 2.5523e-04\n"]}],"source":["# Training\n","history = model.fit(\n","    [X_train_padded, decoder_input_data],  # Encoder input and decoder input\n","    decoder_output_data,                   # Decoder output\n","    epochs=100\n",")"]},{"cell_type":"code","execution_count":null,"id":"c8ba86f5-f75f-44a4-aa0c-1b7673562c63","metadata":{"id":"c8ba86f5-f75f-44a4-aa0c-1b7673562c63"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","def generate_summary(input_text, tokenizer, max_length, model, start_token, end_token):\n","\n","    # Tokenize the input text and pad\n","    input_seq = tokenizer.texts_to_sequences([input_text])\n","    input_seq = pad_sequences(input_seq, maxlen=max_article_length, padding='post')\n","\n","    # Start decoding with the start token\n","    output_seq = [start_token]\n","\n","    for _ in range(max_length):\n","        # Predict next token\n","        predictions = model.predict([input_seq, np.array([output_seq])], verbose=0)\n","\n","        # Get the token with highest probability\n","        next_token = np.argmax(predictions[0, -1, :])\n","\n","        # Stop if end token is generated\n","        if next_token == end_token:\n","            break\n","\n","        # Append the next token to output sequence\n","        output_seq.append(next_token)\n","\n","    # Decode tokens back to text\n","    summary = tokenizer.sequences_to_texts([output_seq])[0]\n","\n","    return summary\n"]},{"cell_type":"code","execution_count":null,"id":"7e26df6b-153b-4a00-a2e1-805c139f6222","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e26df6b-153b-4a00-a2e1-805c139f6222","executionInfo":{"status":"ok","timestamp":1742142075339,"user_tz":240,"elapsed":4498,"user":{"displayName":"Kuldip Mitra","userId":"09334071575411676230"}},"outputId":"7b83b923-52ef-4d24-efb1-39c823d98c04"},"outputs":[{"output_type":"stream","name":"stdout","text":["startofseq jenna jenna are jayden eynaud off her baby bump jayden eynaud was edited out of the show at the 77th ee british academy film awards full season\n"]}],"source":["print(generate_summary(\n","    \"Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? \",\n","    tokenizer=summary_tokenizer,\n","    max_length=50,\n","    model=model,\n","    start_token=summary_tokenizer.word_index[\"startofseq\"],\n","    end_token=summary_tokenizer.word_index[\"endofseq\"]\n","))\n"]},{"cell_type":"code","execution_count":null,"id":"2390a8d0-dd6f-4549-82c5-a52ffb687934","metadata":{"id":"2390a8d0-dd6f-4549-82c5-a52ffb687934"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}